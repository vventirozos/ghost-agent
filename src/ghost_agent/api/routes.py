import datetime
import json
import uuid
import httpx
from fastapi import APIRouter, Request, BackgroundTasks, Security, HTTPException, Depends, Response
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.security.api_key import APIKeyHeader
from ..utils.helpers import get_utc_timestamp
from ..utils.logging import Icons, pretty_log

router = APIRouter()

API_KEY_NAME = "X-Ghost-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

def clear_global_history():
    """
    Placeholder for stateful history clearing. 
    Current implementation is stateless (OpenAI-compatible).
    """
    return True

def get_agent(request: Request):
    return request.app.state.agent

async def verify_api_key(request: Request, api_key: str = Security(api_key_header)):
    agent = get_agent(request)
    if agent.context.args.api_key and (not api_key or api_key != agent.context.args.api_key):
        raise HTTPException(status_code=403, detail="Invalid API Key")
    return api_key

@router.get("/")
async def root_check():
    return Response(content="Ollama is running", media_type="text/plain")

@router.head("/")
async def root_head():
    return Response(content="OK", media_type="text/plain")

@router.get("/api/version")
async def api_version():
    return {"version": "0.1.24"}

@router.post("/api/show")
async def api_show(request: Request):
    return {
        "modelfile": "# Modelfile generated by Ghost Agent\nFROM ghost-model",
        "parameters": "stop \"\n\"",
        "template": "{{ .System }}\nUSER: {{ .Prompt }}\nASSISTANT: ",
        "details": {
            "format": "gguf", "family": "llama", "families": ["llama"], "parameter_size": "7B", "quantization_level": "Q4_0"
        }
    }

@router.get("/api/tags")
async def list_models():
    return {
        "models": [
            {
                "name": "ghost-agent", "model": "ghost-agent", "modified_at": get_utc_timestamp(),
                "size": 1000000000, "digest": "sha256:ghostagent",
                "details": {"format": "gguf", "family": "llama", "families": ["llama"], "parameter_size": "7B", "quantization_level": "Q4_0"}
            }
        ]
    }

@router.get("/v1/models", dependencies=[Security(verify_api_key)])
async def list_openai_models():
    return {
        "object": "list",
        "data": [
            {"id": "ghost-agent", "object": "model", "created": int(datetime.datetime.now().timestamp()), "owned_by": "ghost-system"}
        ]
    }

@router.post("/api/pull")
async def api_pull(request: Request):
    return {"status": "success"}

@router.delete("/api/delete")
async def api_delete(request: Request):
    return {"status": "success"}

@router.post("/api/generate", dependencies=[Security(verify_api_key)])
async def api_generate(request: Request):
    agent = get_agent(request)
    try: body = await request.json()
    except: return JSONResponse({"error": "Invalid JSON"}, 400)

    prompt = body.get("prompt", "")
    model = body.get("model", "default")
    stream = body.get("stream", False)

    chat_payload = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "stream": stream
    }

    try:
        resp = await agent.context.llm_client.http_client.post("/v1/chat/completions", json=chat_payload)
        resp.raise_for_status()
        llm_resp = resp.json()
        content = llm_resp["choices"][0]["message"]["content"]
    except Exception as e:
        return JSONResponse({"error": str(e)}, 500)

    if stream:
         async def generator():
             yield json.dumps({
                 "model": model,
                 "created_at": get_utc_timestamp(),
                 "response": content,
                 "done": True
             }).encode('utf-8') + b"\n"
         return StreamingResponse(generator(), media_type="application/x-ndjson")
    else:
        return {
            "model": model,
            "created_at": get_utc_timestamp(),
            "response": content,
            "done": True
        }

@router.post("/chat", dependencies=[Security(verify_api_key)])
@router.post("/v1/chat/completions", dependencies=[Security(verify_api_key)])
@router.post("/api/chat", dependencies=[Security(verify_api_key)])
async def chat_proxy(request: Request, background_tasks: BackgroundTasks):
    agent = get_agent(request)
    body = await request.json()
    content, created_time, req_id = await agent.handle_chat(body, background_tasks)
    
    if body.get("stream", False):
        return StreamingResponse(agent.context.llm_client.stream_openai(body.get("model", "ghost-agent"), content, created_time, req_id), media_type="text/event-stream")
    
    return JSONResponse({
        "id": f"chatcmpl-{req_id}", "object": "chat.completion", "created": created_time, "model": body.get("model", "ghost-agent"),
        "choices": [{"index": 0, "message": {"role": "assistant", "content": content}, "finish_reason": "stop"}],
        "message": {"role": "assistant", "content": content},
        "done": True, "created_at": get_utc_timestamp()
    })

@router.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "HEAD", "OPTIONS"], dependencies=[Security(verify_api_key)])
async def catch_all(request: Request, path: str):
    agent = get_agent(request)
    url = f"/{path}"
    headers = dict(request.headers)
    headers.pop("host", None)
    headers.pop("content-length", None) 

    try:
        req = agent.context.llm_client.http_client.build_request(
            request.method, url, headers=headers, content=request.stream()
        )
        r = await agent.context.llm_client.http_client.send(req, stream=True)
        return StreamingResponse(r.aiter_bytes(), status_code=r.status_code, media_type=r.headers.get("content-type"))
    except Exception as e:
        return JSONResponse({"error": f"Proxy Error: {e}"}, 502)