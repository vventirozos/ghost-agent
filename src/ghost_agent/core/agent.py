# src/ghost_agent/core/agent.py

import asyncio
import datetime
import json
import logging
import uuid
import re
import gc
import ctypes
import platform
import httpx
from typing import List, Dict, Any, Optional
from pathlib import Path

from .prompts import SYSTEM_PROMPT, CODE_SYSTEM_PROMPT, SMART_MEMORY_PROMPT, PLANNING_SYSTEM_PROMPT
from .planning import TaskTree, TaskStatus
from ..utils.logging import Icons, pretty_log, request_id_context
from ..utils.token_counter import estimate_tokens
from ..tools.registry import get_available_tools, TOOL_DEFINITIONS
from ..tools.tasks import tool_list_tasks
from ..memory.skills import SkillMemory

logger = logging.getLogger("GhostAgent")

def extract_json_from_text(text: str) -> dict:
    """Safely extracts JSON from LLM outputs, ignoring conversational filler and markdown blocks."""
    import re, json
    try:
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', text, re.DOTALL | re.IGNORECASE)
        if match: return json.loads(match.group(1))
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1: return json.loads(text[start:end+1])
        return json.loads(text)
    except Exception:
        return {}

class GhostContext:
    def __init__(self, args, sandbox_dir, memory_dir, tor_proxy):
        self.args = args
        self.sandbox_dir = sandbox_dir
        self.memory_dir = memory_dir
        self.tor_proxy = tor_proxy
        self.llm_client = None
        self.memory_system = None
        self.profile_memory = None
        self.skill_memory = None
        self.scratchpad = None
        self.sandbox_manager = None
        self.scheduler = None
        self.last_activity_time = datetime.datetime.now()
        self.cached_sandbox_state = None

class GhostAgent:
    def __init__(self, context: GhostContext):
        self.context = context
        self.available_tools = get_available_tools(context)
        self.agent_semaphore = asyncio.Semaphore(1)
        self.memory_semaphore = asyncio.Semaphore(1)

    def release_unused_ram(self):
        try:
            gc.collect()
            if platform.system() == "Linux":
                try:
                    libc = ctypes.CDLL("libc.so.6")
                    libc.malloc_trim(0)
                except: pass
        except: pass

    def clear_session(self):
        if hasattr(self.context, 'scratchpad') and self.context.scratchpad:
            self.context.scratchpad.clear()
        self.release_unused_ram()
        return True

    def _prepare_planning_context(self, tools_run_this_turn: List[Dict[str, Any]]) -> str:
        last_tool_output = tools_run_this_turn[-1]["content"] if tools_run_this_turn else "None (Start of Task)"
        if len(last_tool_output) > 5000:
            last_tool_output = last_tool_output[:2500] + "\n...[TRUNCATED]...\n" + last_tool_output[-2500:]
        return last_tool_output

    def _get_recent_transcript(self, messages: List[Dict[str, Any]]) -> str:
        recent_transcript = ""
        transcript_msgs = [m for m in messages if m.get("role") in ["user", "assistant", "tool"]][-10:]
        for m in transcript_msgs:
            content = m.get('content') or ""
            role = m['role'].upper()
            if role == "TOOL":
                role = f"TOOL ({m.get('name', 'unknown')})"
            recent_transcript += f"{role}: {content[:500]}\n"
        return recent_transcript

    def process_rolling_window(self, messages: List[Dict[str, Any]], max_tokens: int) -> List[Dict[str, Any]]:
        if not messages: return []
        system_msgs = [m for m in messages if m.get("role") == "system"]
        raw_history = [m for m in messages if m.get("role") != "system"]
        
        clean_history = []
        seen_tool_outputs = set()
        
        for msg in reversed(raw_history):
            role = msg.get("role")
            content = str(msg.get("content", ""))
            
            if role == "tool":
                tool_name = msg.get('name', 'unknown')
                fingerprint = f"{tool_name}:{content[:100]}"
                if fingerprint in seen_tool_outputs:
                    continue
                seen_tool_outputs.add(fingerprint)
                
            if role == "assistant":
                lower_content = content.lower()
                if ("memory updated" in lower_content or "memory stored" in lower_content) and len(content) < 100:
                    continue
                    
            clean_history.append(msg)
            
        clean_history.reverse()
        compressed_history = []
        msg_count = len(clean_history)
        
        for i, msg in enumerate(clean_history):
            role, content = msg.get("role"), str(msg.get("content", ""))
            if role == "tool" and (msg_count - i) > 5 and len(content) > 5000:
                msg["content"] = content[:2000] + "\n... [OLD DATA COMPRESSED] ...\n" + content[-2000:]
            compressed_history.append(msg)

        current_tokens = sum(estimate_tokens(str(m.get("content", ""))) for m in system_msgs)
        final_history = []
        for msg in reversed(compressed_history):
            msg_tokens = estimate_tokens(str(msg.get("content", "")))
            if current_tokens + msg_tokens > max_tokens: break
            final_history.append(msg)
            current_tokens += msg_tokens
            
        final_history.reverse()
        return system_msgs + final_history

    async def run_smart_memory_task(self, interaction_context: str, model_name: str, selectivity: float):
        if not self.context.memory_system: return
        async with self.memory_semaphore:
            interaction_context = interaction_context.replace("\r", "")
            ic_lower = interaction_context.lower()
            ic_parts = ic_lower.split("ai:")
            user_msg = ic_parts[0] if len(ic_parts) > 0 else ""
            ai_msg = ic_parts[1] if len(ic_parts) > 1 else ""
            
            summary_triggers = ["summarize", "summary", "recall", "tell me about", "what is", "recap", "forget", "list documents"]
            is_requesting_summary = any(w in user_msg for w in summary_triggers)
            
            if is_requesting_summary and len(ai_msg) > 500:
                return
                
            final_prompt = SMART_MEMORY_PROMPT + f"\n{interaction_context}"
            try:
                payload = {"model": model_name, "messages": [{"role": "user", "content": final_prompt}], "stream": False, "temperature": 0.1, "response_format": {"type": "json_object"}}
                data = await self.context.llm_client.chat_completion(payload)
                content = data["choices"][0]["message"]["content"]
                result_json = extract_json_from_text(content)
                score, fact, profile_up = float(result_json.get("score", 0.0)), result_json.get("fact", ""), result_json.get("profile_update", None)
                
                fact_lc = fact.lower()
                is_personal = any(w in fact_lc for w in ["user", "me", "my ", " i ", "identity", "preference", "like"])
                is_technical = any(w in fact_lc for w in ["file", "path", "code", "error", "script", "project", "repo", "build", "library", "version"])
                
                if score >= selectivity and fact and len(fact) <= 200 and len(fact) >= 5 and "none" not in fact_lc:
                    if score >= 0.9 and not (is_personal or is_technical):
                        pretty_log("Auto Memory Skip", f"Discarded generic knowledge: {fact}", icon=Icons.STOP)
                        return
                    memory_type = "identity" if (score >= 0.9 and profile_up) else "auto"
                    await asyncio.to_thread(self.context.memory_system.smart_update, fact, memory_type)
                    pretty_log("Auto Memory Store", f"[{score:.2f}] {fact}", icon=Icons.MEM_SAVE)
                    if memory_type == "identity" and self.context.profile_memory:
                        self.context.profile_memory.update(profile_up.get("category", "notes"), profile_up.get("key", "info"), profile_up.get("value", fact))
            except Exception as e: logger.error(f"Smart memory task failed: {e}")

    async def handle_chat(self, body: Dict[str, Any], background_tasks, request_id: Optional[str] = None):
        req_id = request_id or str(uuid.uuid4())[:8]
        token = request_id_context.set(req_id)
        self.context.last_activity_time = datetime.datetime.now()
        
        try:
            async with self.agent_semaphore:
                pretty_log("Request Initialized", special_marker="BEGIN")
                messages, model, stream_response = body.get("messages", []), body.get("model", "Qwen3-4B-Instruct-2507"), body.get("stream", False)
                
                if len(messages) > 500:
                    messages = [m for m in messages if m.get("role") == "system"] + messages[-500:]
                for m in messages:
                    if isinstance(m.get("content"), str): m["content"] = m["content"].replace("\r", "")
                
                last_user_content = next((m.get("content", "") for m in reversed(messages) if m.get("role") == "user"), "")
                lc = last_user_content.lower()
                
                coding_keywords = ["python", "bash", "sh", "script", "code", "def ", "import "]
                coding_actions = ["write", "run", "execute", "debug", "fix", "create", "generate", "count", "calculate", "analyze", "scrape", "plot", "graph"]
                has_coding_intent = False
                
                if any(k in lc for k in coding_keywords):
                    if any(a in lc for a in coding_actions): has_coding_intent = True
                if any(x in lc for x in ["execute", "script", ".py"]): has_coding_intent = True
                if re.match(r'^[\d\s\+\-\*\/\(\)\=\?]+$', lc):
                    has_coding_intent = False
                    
                profile_context = self.context.profile_memory.get_context_string() if self.context.profile_memory else ""
                profile_context = profile_context.replace("\r", "")
                
                scratch_data = self.context.scratchpad.list_all() if hasattr(self.context, 'scratchpad') else "None."
                working_memory_context = f"\n\n### SCRAPBOOK (Persistent Data):\n{scratch_data}\n\n"
                
                if has_coding_intent:
                    base_prompt, current_temp = CODE_SYSTEM_PROMPT, 0.2
                    pretty_log("Mode Switch", "Ghost Python Specialist Activated", icon=Icons.TOOL_CODE)
                    if profile_context: base_prompt = base_prompt.replace("{{PROFILE}}", profile_context)
                else:
                    base_prompt, current_temp = SYSTEM_PROMPT.replace("{{PROFILE}}", profile_context), self.context.args.temperature
                    
                base_prompt += working_memory_context
                base_prompt = base_prompt.replace("{{CURRENT_TIME}}", datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')).replace("\r", "")
                
                found_system = False
                for m in messages:
                    if m.get("role") == "system": m["content"] = base_prompt; found_system = True; break
                if not found_system: messages.insert(0, {"role": "system", "content": base_prompt})
                
                if "task" in lc and ("list" in lc or "show" in lc or "what" in lc or "status" in lc):
                     current_tasks = await tool_list_tasks(self.context.scheduler)
                     messages.append({"role": "system", "content": f"SYSTEM DATA DUMP:\n{current_tasks}\n\nINSTRUCTION: The user cannot see the data above. You MUST copy the task list into your **FINAL ANSWER** now."})
                
                is_fact_check = "fact-check" in lc or "verify" in lc
                trivial_triggers = ["who are you", "hello", " hi ", "hey there", "how are you", "what's up", "name is"]
                is_trivial = any(t in last_user_content.lower() for t in trivial_triggers)
                
                should_fetch_memory = (
                    not is_fact_check and 
                    not is_trivial and
                    (not has_coding_intent or "remember" in last_user_content or "previous" in last_user_content)
                )
                
                if self.context.memory_system and last_user_content and should_fetch_memory:
                    mem_context = self.context.memory_system.search(last_user_content)
                    if mem_context:
                        mem_context = mem_context.replace("\r", "")
                        pretty_log("Memory Context", f"Retrieved for: {last_user_content}", icon=Icons.BRAIN_CTX)
                        messages.insert(1, {"role": "system", "content": f"[MEMORY CONTEXT]:\n{mem_context}"})
                        
                if self.context.skill_memory:
                    playbook = self.context.skill_memory.get_playbook_context(query=last_user_content, memory_system=self.context.memory_system)
                    for m in messages:
                        if m.get("role") == "system":
                            m["content"] += f"\n\n{playbook}"
                            break
                            
                messages = self.process_rolling_window(messages, self.context.args.max_context)
                
                final_ai_content, created_time = "", int(datetime.datetime.now().timestamp())
                force_stop, seen_tools, tool_usage, last_was_failure = False, set(), {}, False
                raw_tools_called = set()
                execution_failure_count = 0
                tools_run_this_turn = []
                forget_was_called = False
                thought_content = ""
                was_complex_task = False
                
                task_tree = TaskTree()
                current_plan_json = {}
                
                for turn in range(20):
                    if turn > 2: was_complex_task = True
                    if force_stop: break
                    
                    use_plan = getattr(self.context.args, 'use_planning', True)
                    if use_plan and not is_trivial:
                        pretty_log("Reasoning Loop", f"Turn {turn+1} Strategic Analysis...", icon=Icons.BRAIN_PLAN)
                        
                        last_tool_output = self._prepare_planning_context(tools_run_this_turn)
                        recent_transcript = self._get_recent_transcript(messages)
                            
                        planning_prompt = f""" ### CURRENT SITUATION
### RECENT CONVERSATION:
{recent_transcript}
User Request: {last_user_content}
Last Tool Output: {last_tool_output}
### CURRENT PLAN (JSON)
{json.dumps(current_plan_json, indent=2) if current_plan_json else "No plan yet."}
"""
                        planning_payload = {
                            "model": model,
                            "messages": [{"role": "system", "content": PLANNING_SYSTEM_PROMPT}, {"role": "user", "content": planning_prompt}],
                            "temperature": 0.1,
                            "max_tokens": 1024,
                            "response_format": {"type": "json_object"}
                        }
                        
                        try:
                            p_data = await self.context.llm_client.chat_completion(planning_payload)
                            plan_content = p_data["choices"][0]["message"].get("content", "")
                            plan_json = extract_json_from_text(plan_content)
                            
                            thought_content = plan_json.get("thought", "No thought provided.")
                            tree_update = plan_json.get("tree_update", {})
                            next_action_id = plan_json.get("next_action_id", "")
                            
                            if tree_update:
                                task_tree.load_from_json(tree_update)
                                current_plan_json = task_tree.to_json()
                                
                            tree_render = task_tree.render()
                            
                            found_checklist = False
                            for m in messages:
                                if m.get("role") == "system" and "### ACTIVE STRATEGY" in m.get("content", ""):
                                    m["content"] = f"### ACTIVE STRATEGY & PLAN (DO NOT SKIP ANY STEP):\nTHOUGHT: {thought_content}\n\nPLAN:\n{tree_render}\n\nFOCUS TASK: {next_action_id}"
                                    found_checklist = True
                                    break
                            if not found_checklist:
                                messages.append({"role": "system", "content": f"### ACTIVE STRATEGY & PLAN (DO NOT SKIP ANY STEP):\nTHOUGHT: {thought_content}\n\nPLAN:\n{tree_render}\n\nFOCUS TASK: {next_action_id}"})
                            
                            pretty_log("INTERNAL MONOLOGUE", icon=Icons.BRAIN_THINK, special_marker="SECTION_START")
                            pretty_log("Planner Monologue", thought_content, icon=Icons.BRAIN_THINK)
                            pretty_log("INTERNAL MONOLOGUE", icon=Icons.BRAIN_THINK, special_marker="SECTION_END")
                            pretty_log("Reasoning Loop", f"Plan Updated. Focus: {next_action_id}", icon=Icons.OK)
                            
                            if task_tree.root_id and task_tree.nodes[task_tree.root_id].status == TaskStatus.DONE and turn > 0:
                                pretty_log("Finalizing", "Agent signaled completion", icon=Icons.OK)
                        except Exception as e:
                            logger.error(f"Planning step failed: {e}")
                            if not any("### ACTIVE STRATEGY" in m.get("content", "") for m in messages):
                                messages.append({"role": "system", "content": "### ACTIVE STRATEGY: Proceed with the next logical step to fulfill the user request."})

                    scratch_data = self.context.scratchpad.list_all() if hasattr(self.context, 'scratchpad') else "None."
                    
                    if has_coding_intent:
                        if self.context.cached_sandbox_state is None:
                            from ..tools.file_system import tool_list_files
                            params = {
                                "sandbox_dir": self.context.sandbox_dir, 
                                "memory_system": self.context.memory_system
                            }
                            sandbox_state = await asyncio.to_thread(lambda: asyncio.run(tool_list_files(**params)) if asyncio.iscoroutinefunction(tool_list_files) else tool_list_files(**params))
                            self.context.cached_sandbox_state = sandbox_state
                        else:
                            sandbox_state = self.context.cached_sandbox_state
                            
                        for m in messages:
                            if m.get("role") == "system":
                                content = re.sub(r'\n### CURRENT SANDBOX STATE \(Eyes-On\):.*?\n\n', '\n', m["content"], flags=re.DOTALL)
                                content += f"\n### CURRENT SANDBOX STATE (Eyes-On):\n{sandbox_state}\n\n"
                                content = re.sub(r'\n### SCRAPBOOK \(Persistent Data\):.*?\n\n', '\n', content, flags=re.DOTALL)
                                content += f"\n### SCRAPBOOK (Persistent Data):\n{scratch_data}\n\n"
                                m["content"] = content
                                break
                    else:
                        for m in messages:
                            if m.get("role") == "system":
                                m["content"] = re.sub(r'\n### SCRAPBOOK \(Persistent Data\):.*?\n\n', '\n', m["content"], flags=re.DOTALL)
                                m["content"] += f"\n### SCRAPBOOK (Persistent Data):\n{scratch_data}\n\n"
                                break

                    if last_was_failure:
                        if execution_failure_count == 1:
                            active_temp = max(current_temp, 0.40)
                        elif execution_failure_count >= 2:
                            active_temp = max(current_temp, 0.60)
                        else:
                            active_temp = min(current_temp + 0.1, 0.80)
                        pretty_log("Brainstorming", f"Adjusting variance to {active_temp:.2f} to solve error", icon=Icons.IDEA)
                    else:
                        active_temp = current_temp

                    payload = {
                        "model": model, 
                        "messages": messages, 
                        "stream": False, 
                        "tools": TOOL_DEFINITIONS, 
                        "tool_choice": "auto", 
                        "temperature": active_temp, 
                        "frequency_penalty": 0.5,
                        "max_tokens": 4096
                    }
                    
                    pretty_log("LLM Request", f"Turn {turn+1} | Temp {active_temp:.2f}", icon=Icons.LLM_ASK)
                    
                    # Ensure msg is always defined in this scope
                    msg = {"role": "assistant", "content": "", "tool_calls": []}
                    try:
                        data = await self.context.llm_client.chat_completion(payload)
                        if "choices" in data and len(data["choices"]) > 0:
                            msg = data["choices"][0]["message"]
                    except (httpx.ConnectError, httpx.ConnectTimeout):
                        final_ai_content = "CRITICAL: The upstream LLM server is unreachable. It may have crashed due to memory pressure or is currently restarting. Please wait a moment and try again."
                        pretty_log("System Fault", "Upstream server unreachable", level="ERROR", icon=Icons.FAIL)
                        force_stop = True
                        break
                    except Exception as e:
                        final_ai_content = f"CRITICAL: An unexpected error occurred while communicating with the LLM: {str(e)}"
                        pretty_log("System Fault", str(e), level="ERROR", icon=Icons.FAIL)
                        force_stop = True
                        break

                    content = msg.get("content") or ""
                    tool_calls = list(msg.get("tool_calls") or [])
                    
                    # ---------------------------------------------------------
                    # üõ†Ô∏è THE QWEN SYNTAX HEALER & SCRUBBER
                    # ---------------------------------------------------------
                    if "<tool_call>" in content:
                        pretty_log("Syntax Healer", "Intercepted leaked <tool_call> tags. Repairing...", icon=Icons.SHIELD)
                        
                        # Only try to manually parse if the backend completely missed it
                        if not tool_calls:
                            matches = re.findall(r'<tool_call>\s*(\{.*?\})\s*</tool_call>', content, re.DOTALL | re.IGNORECASE)
                            for match in matches:
                                try:
                                    t_data = extract_json_from_text(match)
                                    if t_data and "name" in t_data:
                                        tool_calls.append({
                                            "id": f"call_{uuid.uuid4().hex[:8]}",
                                            "type": "function",
                                            "function": {
                                                "name": t_data.get("name"),
                                                "arguments": json.dumps(t_data.get("arguments", {}))
                                            }
                                        })
                                except Exception: pass
                                
                        # Radically erase the raw syntax so it doesn't pollute the user's chat output
                        content = re.sub(r'<tool_call>.*?</tool_call>', '', content, flags=re.DOTALL | re.IGNORECASE).strip()
                    # ---------------------------------------------------------

                    if content:
                        content = content.replace("\r", "")
                        if final_ai_content and not final_ai_content.endswith("\n\n"):
                            final_ai_content += "\n\n"
                        final_ai_content += content
                        msg["content"] = content
                    else:
                        msg["content"] = ""
                        
                    msg["tool_calls"] = tool_calls
                    
                    if not tool_calls:
                        user_request_context = last_user_content.lower()
                        has_meta_intent = any(kw in user_request_context for kw in ["learn", "skill", "profile", "lesson", "playbook", "record", "save"])
                        meta_tools_called = any(t in raw_tools_called for t in ["learn_skill", "update_profile"])
                        
                        if has_meta_intent and not meta_tools_called and turn < 4:
                            pretty_log("Checklist Nudge", "Enforcing meta-task compliance", icon=Icons.SHIELD)
                            # Remove the recently added content to prevent duplicating text during the loop
                            if content:
                                final_ai_content = final_ai_content[:-len(content)].strip()
                            messages.append({"role": "system", "content": "CRITICAL: You have not fulfilled the learning/profile instructions in the user's request. You MUST call 'learn_skill' or 'update_profile' now before finishing."})
                            continue

                        if self.context.args.smart_memory > 0.0 and last_user_content and not forget_was_called and not last_was_failure:
                            background_tasks.add_task(self.run_smart_memory_task, f"User: {last_user_content}\nAI: {final_ai_content}", model, self.context.args.smart_memory)
                        break
                        
                    messages.append(msg)
                    last_was_failure = False
                    redundancy_strikes = 0
                    
                    tool_tasks, tool_call_metadata = [], []
                    for tool in tool_calls:
                        fname = tool["function"]["name"]
                        raw_tools_called.add(fname)
                        tool_usage[fname] = tool_usage.get(fname, 0) + 1
                        
                        if fname in ["write_file", "delete_file", "download_file", "git_clone", "unzip", "move_file", "copy_file", "execute"]:
                            self.context.cached_sandbox_state = None
                            
                        if fname == "forget":
                            forget_was_called = True
                        elif fname == "knowledge_base":
                            try:
                                args = json.loads(tool["function"]["arguments"])
                                if args.get("action") == "forget":
                                    forget_was_called = True
                            except: pass

                        if tool_usage[fname] > (20 if fname == "execute" else 10):
                            pretty_log("Loop Breaker", f"Halted overuse: {fname}", icon=Icons.STOP)
                            messages.append({"role": "system", "content": f"SYSTEM: Tool '{fname}' used too many times."})
                            force_stop = True; break

                        try:
                            t_args = json.loads(tool["function"]["arguments"])
                            a_hash = f"{fname}:{json.dumps(t_args, sort_keys=True)}"
                        except: t_args, a_hash = {}, f"{fname}:error"
                        
                        is_state_tool = fname in ["file_system", "knowledge_base", "web_search", "recall", "list_files", "system_utility", "inspect_file", "manage_tasks"]
                        
                        if a_hash in seen_tools and fname != "execute" and not is_state_tool:
                            redundancy_strikes += 1
                            pretty_log("Redundancy", f"Blocked duplicate: {fname}", icon=Icons.RETRY)
                            messages.append({"role": "tool", "tool_call_id": tool["id"], "name": fname, "content": "SYSTEM MONITOR: Already executed successfully. Do not repeat this tool call. If you have finished all other tasks, provide your final response now."})
                            if redundancy_strikes >= 3: force_stop = True
                            continue
                            
                        seen_tools.add(a_hash)
                        
                        if fname == "execute":
                            code_content = t_args.get("content", "")
                            if len(code_content.splitlines()) > 10:
                                pretty_log("Red Team Audit", "Reviewing complex code for destructive risk...", icon=Icons.SHIELD)
                                is_approved, revised_code, critique = await self._run_critic_check(code_content, last_user_content, model)
                                
                                if not is_approved and revised_code:
                                    pretty_log("Red Team Intervention", "Code patched for safety/logic.", icon=Icons.SHIELD)
                                    t_args["content"] = revised_code
                                    tool["function"]["arguments"] = json.dumps(t_args)
                                    messages.append({"role": "system", "content": f"RED TEAM INTERVENTION: Your code was auto-corrected before execution.\nCritique: {critique}\nExecuting patched version."})
                                elif not is_approved:
                                    pretty_log("Red Team Block", f"{critique}", icon=Icons.SHIELD)
                                    messages.append({"role": "tool", "tool_call_id": tool["id"], "name": fname, "content": f"RED TEAM BLOCK: {critique}. Rewrite the code."})
                                    last_was_failure = True
                                    continue

                        if fname in self.available_tools:
                            tool_tasks.append(self.available_tools[fname](**t_args))
                            tool_call_metadata.append((fname, tool["id"], a_hash))
                        else: messages.append({"role": "tool", "tool_call_id": tool["id"], "name": fname, "content": "Error: Unknown tool"})

                    if tool_tasks:
                        results = await asyncio.gather(*tool_tasks, return_exceptions=True)
                        for i, result in enumerate(results):
                            fname, tool_id, a_hash = tool_call_metadata[i]
                            str_res = str(result).replace("\r", "") if not isinstance(result, Exception) else f"Error: {str(result)}"
                            safe_res = str_res[:2000] + "\n...[TRUNCATED]...\n" + str_res[-2000:] if len(str_res) > 4000 else str_res
                            tool_msg = {"role": "tool", "tool_call_id": tool_id, "name": fname, "content": safe_res}
                            messages.append(tool_msg)
                            tools_run_this_turn.append(tool_msg)
                            
                            if fname == "execute":
                                code_match = re.search(r"EXIT CODE:\s*(\d+)", str_res)
                                if code_match:
                                    exit_code_val = int(code_match.group(1))
                                else:
                                    if "Error" in str_res or "Exception" in str_res or "Traceback" in str_res:
                                        exit_code_val = 1
                                    else:
                                        exit_code_val = 0
                                        
                                if exit_code_val != 0:
                                    execution_failure_count += 1
                                    last_was_failure = True
                                    
                                    error_preview = "Unknown Error"
                                    if "STDOUT/STDERR:" in str_res:
                                        error_preview = str_res.split("STDOUT/STDERR:")[1].strip().replace("\n", " ")
                                    elif "SYSTEM ERROR:" in str_res:
                                        error_preview = str_res.split("SYSTEM ERROR:")[1].strip().split("\n")[0]
                                    else:
                                        error_preview = str_res[:60].replace("\n", " ")
                                        
                                    pretty_log("Execution Fail", f"Strike {execution_failure_count}/3 -> {error_preview}", icon=Icons.FAIL)
                                    from ..tools.file_system import tool_list_files
                                    sandbox_state = await tool_list_files(self.context.sandbox_dir, self.context.memory_system)
                                    messages.append({"role": "system", "content": f"AUTO-DIAGNOSTIC: The script failed. SANDBOX TREE:\n{sandbox_state}"})
                                    if execution_failure_count >= 3: force_stop = True
                                else:
                                    execution_failure_count = 0
                                    pretty_log("Execution Ok", "Script completed with exit code 0", icon=Icons.OK)
                                    request_context = (last_user_content + thought_content).lower()
                                    has_meta_intent = any(kw in request_context for kw in ["learn", "skill", "profile", "lesson", "playbook", "record", "save"])
                                    if not has_meta_intent:
                                        force_stop = True
                                        
                            elif str_res.startswith("Error:") or str_res.startswith("Critical Tool Error"):
                                last_was_failure = True
                                if not force_stop:
                                    error_preview = str_res.replace("Error:", "").strip()
                                    pretty_log("Tool Warning", f"{fname} -> {error_preview}", icon=Icons.WARN)
                                    
                            elif fname in ["manage_tasks", "learn_skill", "update_profile"] and "SUCCESS" in str_res.upper():
                                # Let the agent naturally answer the user instead of halting abruptly.
                                pass

                # --- THE "PERFECT IT" PROTOCOL INJECTION ---
                # Only trigger proactive optimization for heavy engineering/research tasks
                heavy_tools_used = any(t.get('name') in ['execute', 'deep_research'] for t in tools_run_this_turn)
                
                if tools_run_this_turn and heavy_tools_used and (not final_ai_content or len(final_ai_content) < 50):
                    pretty_log("Perfect It Protocol", "Generating proactive optimization...", icon=Icons.IDEA)
                    perfect_it_prompt = f"Task completed successfully. Final tool output:\n\n{tools_run_this_turn[-1]['content']}\n\n<system_directive>First, succinctly present the tool output/result to the user. Then, based on your Perfection Protocol, analyze the result and proactively suggest one concrete way to optimize, scale, secure, or automate this work further. RESPOND IN PLAIN TEXT ONLY. DO NOT USE TOOLS.</system_directive>"
                    messages.append({"role": "system", "content": perfect_it_prompt})
                    
                    payload["messages"] = messages
                    
                    # üî¥ CRITICAL FIX: Physically remove tools from payload so it cannot hallucinate a tool call
                    if "tools" in payload: del payload["tools"]
                    if "tool_choice" in payload: del payload["tool_choice"]
                    
                    try:
                        perfection_data = await self.context.llm_client.chat_completion(payload)
                        p_msg = perfection_data["choices"][0]["message"].get("content", "")
                        p_msg = re.sub(r'<tool_call>.*?</tool_call>', '', p_msg, flags=re.DOTALL | re.IGNORECASE).strip()
                        if final_ai_content:
                            final_ai_content += "\n\n" + p_msg
                        else:
                            final_ai_content = p_msg
                    except Exception:
                        if not final_ai_content:
                            final_ai_content = "Task finished successfully, but optimization generation failed."
                elif tools_run_this_turn and not final_ai_content: 
                    final_ai_content = "Process finished successfully."

                # --- FINAL OUTPUT SCRUBBER ---
                final_ai_content = re.sub(r'<tool_call>.*?</tool_call>', '', final_ai_content, flags=re.DOTALL | re.IGNORECASE).strip()
                if not final_ai_content:
                    final_ai_content = "Task executed successfully."

                # --- AUTOMATED POST-MORTEM (AUTO-LEARNING) ---
                if was_complex_task or execution_failure_count > 0:
                    try:
                        if not force_stop or "READY TO FINALIZE" in thought_content.upper():
                            history_summary = f"User: {last_user_content}\n"
                            for t_msg in tools_run_this_turn[-5:]:
                                history_summary += f"Tool {t_msg['name']}: {t_msg['content'][:200]}\n"
                                
                            learn_prompt = f"### TASK POST-MORTEM\nReview this successful but complex interaction. Did the agent encounter a specific error, hurdle, or mistake that required a unique solution? If so, extract it as a lesson.\n\nHISTORY:\n{history_summary}\n\nFINAL AI: {final_ai_content[:500]}\n\nReturn ONLY a JSON object with 'task', 'mistake', and 'solution'. If no unique lesson is found, return null."
                            
                            payload = {"model": model, "messages": [{"role": "system", "content": "You are a Meta-Cognitive Analyst."}, {"role": "user", "content": learn_prompt}], "temperature": 0.1, "response_format": {"type": "json_object"}}
                            l_data = await self.context.llm_client.chat_completion(payload)
                            l_content = l_data["choices"][0]["message"].get("content", "")
                            if l_content and "null" not in l_content.lower():
                                l_json = extract_json_from_text(l_content)
                                if all(k in l_json for k in ["task", "mistake", "solution"]):
                                    self.context.skill_memory.learn_lesson(l_json["task"], l_json["mistake"], l_json["solution"], memory_system=self.context.memory_system)
                                    pretty_log("Auto-Learning", "New lesson captured automatically", icon=Icons.IDEA)
                    except Exception as e:
                        logger.error(f"Auto-learning failed: {e}")

                return final_ai_content, created_time, req_id
                
        finally:
            if 'messages' in locals(): del messages
            if 'tools_run_this_turn' in locals(): del tools_run_this_turn
            if 'sandbox_state' in locals(): del sandbox_state
            if 'data' in locals(): del data
            
            pretty_log("Request Finished", special_marker="END")
            request_id_context.reset(token)

    async def _run_critic_check(self, code: str, task_context: str, model: str):
        from .prompts import CRITIC_SYSTEM_PROMPT
        try:
            prompt = f"### USER TASK:\n{task_context}\n\n### PROPOSED CODE:\n{code}"
            payload = {
                "model": model, 
                "messages": [{"role": "system", "content": CRITIC_SYSTEM_PROMPT}, {"role": "user", "content": prompt}], 
                "temperature": 0.0,
                "response_format": {"type": "json_object"}
            }
            data = await self.context.llm_client.chat_completion(payload)
            content = data["choices"][0]["message"]["content"]
            result = extract_json_from_text(content)
            
            if result.get("status") == "APPROVED":
                return True, None, "Approved"
            else:
                revised_code = result.get("revised_code")
                if revised_code:
                    from ..utils.sanitizer import extract_code_from_markdown
                    revised_code = extract_code_from_markdown(revised_code)
                    
                    # Double-check for leaked backticks or inline code style (if extract failed to strip them)
                    if revised_code.startswith("`") and revised_code.endswith("`"):
                        revised_code = revised_code.strip("`")
                return False, revised_code, result.get("critique", "Unspecified issue")
                
        except Exception as e:
            logger.error(f"Critic failed: {e}")
            return True, None, "Critic Failed (Fail-Open)"
            